 Backport from 2.6 upstream
 8df09ea3b8ccfe0c94844102d33fa46f57c08d9e [SOCK] Avoid integer divides where not necessary in include/net/sock.h
 21371f768bf7127ee45bfaadd17899df6a439e8f [SOCK] Avoid divides in sk_stream_pages() and __sk_stream_mem_reclaim()

---
 include/net/sock.h |    2 +-
 include/net/tcp.h  |   33 +++++++++++++++++++++------------
 net/ipv4/tcp.c     |   22 +++++++++++++---------
 3 files changed, 35 insertions(+), 22 deletions(-)

diff -urBp a/include/net/sock.h b/include/net/sock.h
--- a/include/net/sock.h	2010-08-21 18:42:40.000000000 +0400
+++ b/include/net/sock.h	2010-09-23 18:02:25.000000000 +0400
@@ -1404,7 +1404,7 @@ static inline void sk_wake_async(struct 
  */
 static inline int sock_writeable(struct sock *sk) 
 {
-	return atomic_read(&sk->wmem_alloc) < (sk->sndbuf / 2);
+	return atomic_read(&sk->wmem_alloc) < (sk->sndbuf >> 1);
 }
 
 static inline int gfp_any(void)
diff -urBp a/include/net/tcp.h b/include/net/tcp.h
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1107,7 +1107,7 @@ struct tcp_skb_cb {
  */
 static inline int tcp_min_write_space(struct sock *sk)
 {
-	return sk->wmem_queued/2;
+	return sk->wmem_queued >> 1;
 }
  
 static inline int tcp_wspace(struct sock *sk)
@@ -1844,6 +1844,7 @@ static __inline__ void tcp_openreq_init(
 }
 
 #define TCP_MEM_QUANTUM	((int)PAGE_SIZE)
+#define TCP_MEM_QUANTUM_SHIFT ilog2(TCP_MEM_QUANTUM)
 
 static inline void tcp_free_skb(struct sock *sk, struct sk_buff *skb)
 {
@@ -1885,7 +1886,7 @@ static inline void tcp_enter_memory_pres
 static inline void tcp_moderate_sndbuf(struct sock *sk)
 {
 	if (!(sk->userlocks&SOCK_SNDBUF_LOCK)) {
-		sk->sndbuf = min(sk->sndbuf, sk->wmem_queued/2);
+		sk->sndbuf = min(sk->sndbuf, sk->wmem_queued >> 1);
 		sk->sndbuf = max(sk->sndbuf, SOCK_MIN_SNDBUF);
 	}
 }
diff -urBp a/net/ipv4/tcp.c b/net/ipv4/tcp.c
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -285,13 +285,13 @@ atomic_t tcp_sockets_allocated;	/* Curre
  * is strict, actions are advisory and have some latency. */
 int tcp_memory_pressure;
 
-#define TCP_PAGES(amt) (((amt)+TCP_MEM_QUANTUM-1)/TCP_MEM_QUANTUM)
+#define TCP_PAGES(amt) (((amt)+TCP_MEM_QUANTUM-1) >> TCP_MEM_QUANTUM_SHIFT)
 
 int tcp_mem_schedule(struct sock *sk, int size, int kind)
 {
 	int amt = TCP_PAGES(size);
 
-	sk->forward_alloc += amt*TCP_MEM_QUANTUM;
+	sk->forward_alloc += amt << TCP_MEM_QUANTUM_SHIFT;
 	atomic_add(amt, &tcp_memory_allocated);
 
 	/* Under limit. */
@@ -338,7 +338,7 @@ suppress_allocation:
 	}
 
 	/* Alas. Undo changes. */
-	sk->forward_alloc -= amt*TCP_MEM_QUANTUM;
+	sk->forward_alloc -= amt << TCP_MEM_QUANTUM_SHIFT;
 	atomic_sub(amt, &tcp_memory_allocated);
 	return 0;
 }
@@ -346,7 +346,7 @@ suppress_allocation:
 void __tcp_mem_reclaim(struct sock *sk)
 {
 	if (sk->forward_alloc >= TCP_MEM_QUANTUM) {
-		atomic_sub(sk->forward_alloc/TCP_MEM_QUANTUM, &tcp_memory_allocated);
+		atomic_sub(sk->forward_alloc >> TCP_MEM_QUANTUM_SHIFT, &tcp_memory_allocated);
 		sk->forward_alloc &= (TCP_MEM_QUANTUM-1);
 		if (tcp_memory_pressure &&
 		    atomic_read(&tcp_memory_allocated) < sysctl_tcp_mem[0])
-- 
