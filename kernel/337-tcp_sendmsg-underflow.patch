 Backport of
 ef015786152adaff5a6a8bf0c8ea2f70cee8059d [TCP]: Fix sk_forward_alloc underflow in tcp_sendmsg

From: Herbert Xu
Date: Thu, 1 Sep 2005 17:48:59 -0700

I've finally found a potential cause of the sk_forward_alloc underflows
that people have been reporting sporadically.

When tcp_sendmsg tacks on extra bits to an existing TCP_PAGE we don't
check sk_forward_alloc even though a large amount of time may have
elapsed since we allocated the page.  In the mean time someone could've
come along and liberated packets and reclaimed sk_forward_alloc memory.

This patch makes tcp_sendmsg check sk_forward_alloc every time as we
do in do_tcp_sendpages.

Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
Signed-off-by: David S. Miller <davem@davemloft.net>
---

 include/net/tcp.h |   28 ++++++++++++++++++----------
 net/ipv4/tcp.c    |   14 +++++++++-----
 2 files changed, 27 insertions(+), 15 deletions(-)

diff -urBp a/include/net/tcp.h b/include/net/tcp.h
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1876,6 +1876,12 @@ static inline void tcp_enter_memory_pres
 	}
 }
 
+static inline int tcp_wmem_schedule(struct sock *sk, int size)
+{
+	return size <= sk->forward_alloc ||
+		tcp_mem_schedule(sk, size, 0);
+}
+
 static inline void tcp_moderate_sndbuf(struct sock *sk)
 {
 	if (!(sk->userlocks&SOCK_SNDBUF_LOCK)) {
@@ -1890,8 +1896,11 @@ static inline struct sk_buff *tcp_alloc_
 
 	if (skb) {
 		skb->truesize += mem;
-		if (sk->forward_alloc >= (int)skb->truesize ||
-		    tcp_mem_schedule(sk, skb->truesize, 0)) {
+		if (tcp_wmem_schedule(sk, skb->truesize)) {
+			/*
+			 * Make sure that we have exactly size bytes
+			 * available to the caller, no more, no less.
+			 */
 			skb_reserve(skb, MAX_TCP_HEADER);
 			return skb;
 		}
@@ -1910,15 +1919,14 @@ static inline struct sk_buff *tcp_alloc_
 
 static inline struct page * tcp_alloc_page(struct sock *sk)
 {
-	if (sk->forward_alloc >= (int)PAGE_SIZE ||
-	    tcp_mem_schedule(sk, PAGE_SIZE, 0)) {
-		struct page *page = alloc_pages(sk->allocation, 0);
-		if (page)
-			return page;
+	struct page *page;
+
+	page = alloc_pages(sk->allocation, 0);
+	if (!page) {
+		tcp_enter_memory_pressure();
+		tcp_moderate_sndbuf(sk);
 	}
-	tcp_enter_memory_pressure();
-	tcp_moderate_sndbuf(sk);
-	return NULL;
+	return page;
 }
 
 static inline void tcp_writequeue_purge(struct sock *sk)
diff -urBp a/net/ipv4/tcp.c b/net/ipv4/tcp.c
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -1112,19 +1112,23 @@ new_segment:
 					if (off == PAGE_SIZE) {
 						put_page(page);
 						TCP_PAGE(sk) = page = NULL;
+						off = 0;
 					}
-				}
+				} else
+					off = 0;
+
+				if (copy > PAGE_SIZE - off)
+					copy = PAGE_SIZE - off;
+
+				if (!tcp_wmem_schedule(sk, copy))
+					goto wait_for_memory;
 
 				if (!page) {
 					/* Allocate new cache page. */
 					if (!(page=tcp_alloc_page(sk)))
 						goto wait_for_memory;
-					off = 0;
 				}
 
-				if (copy > PAGE_SIZE-off)
-					copy = PAGE_SIZE-off;
-
 				/* Time to copy data. We are close to the end! */
 				err = tcp_copy_to_page(sk, from, skb, page, off, copy);
 				if (err) {
-- 
1.7.3
