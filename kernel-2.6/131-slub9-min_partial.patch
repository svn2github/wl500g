From 654e97b3c39bad4b5202429f727f89f16f60b49c
From: Fedor <fedork@ubuntu.(none)>
Date: Tue, 19 Oct 2010 23:27:58 -0400
Subject: [PATCH] slub: add min_partial sysfs tunable

 kernel.org commits
 5595cffc8248e4672c5803547445e85e4053c8fc SLUB: dynamic per-cache MIN_PARTIAL
 3b89d7d881a1dbb4da158f7eb5d6b3ceefc72810 slub: move min_partial to struct kmem_cache
 73d342b169db700b5a6ad626fe4b86911efec8db slub: add min_partial sysfs tunable
 c0bdb232b23b51c23e551041510ad6bea5ce5a92 slub: rename calculate_min_partial() to set_min_partial()

---
 include/linux/slub_def.h |    1 +
 mm/slub.c                |   48 +++++++++++++++++---
 2 files changed, 43 insertions(+), 6 deletions(-)

diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
index e34c303..08ec34b 100644
--- a/include/linux/slub_def.h
+++ b/include/linux/slub_def.h
@@ -43,6 +43,7 @@ struct kmem_cache {
 	void (*ctor)(void *, struct kmem_cache *, unsigned long);
 	int inuse;		/* Offset to metadata */
 	int align;		/* Alignment */
+	unsigned long min_partial;
 	const char *name;	/* Name (only for display!) */
 	struct list_head list;	/* List of slab caches */
 	struct kobject kobj;	/* For sysfs */
diff --git a/mm/slub.c b/mm/slub.c
index 599bfcf..2cddb9a 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1231,7 +1231,7 @@ static struct page *get_any_partial(struct kmem_cache *s, gfp_t flags)
 		n = get_node(s, zone_to_nid(*z));
 
 		if (n && cpuset_zone_allowed_hardwall(*z, flags) &&
-				n->nr_partial > MIN_PARTIAL) {
+				n->nr_partial > s->min_partial) {
 			page = get_partial_node(n);
 			if (page)
 				return page;
@@ -1277,7 +1277,7 @@ static void unfreeze_slab(struct kmem_cache *s, struct page *page, int tail)
 		slab_unlock(page);
 
 	} else {
-		if (n->nr_partial < MIN_PARTIAL) {
+		if (n->nr_partial < s->min_partial) {
 			/*
 			 * Adding an empty slab to the partial slabs in order
 			 * to avoid page allocator overhead. This slab needs
@@ -1778,7 +1778,8 @@ static unsigned long calculate_alignment(unsigned long flags,
 	return ALIGN(align, sizeof(void *));
 }
 
-static void init_kmem_cache_node(struct kmem_cache_node *n)
+static void
+init_kmem_cache_node(struct kmem_cache_node *n, struct kmem_cache *s)
 {
 	n->nr_partial = 0;
 	atomic_long_set(&n->nr_slabs, 0);
@@ -1814,7 +1815,7 @@ static struct kmem_cache_node * __init early_kmem_cache_node_alloc(gfp_t gfpflag
 	page->inuse++;
 	kmalloc_caches->node[node] = n;
 	setup_object_debug(kmalloc_caches, page, n);
-	init_kmem_cache_node(n);
+	init_kmem_cache_node(n, kmalloc_caches);
 	atomic_long_inc(&n->nr_slabs);
 	/*
 	 * lockdep requires consistent irq usage for each lock
@@ -1870,7 +1871,7 @@ static int init_kmem_cache_nodes(struct kmem_cache *s, gfp_t gfpflags)
 
 		}
 		s->node[node] = n;
-		init_kmem_cache_node(n);
+		init_kmem_cache_node(n, s);
 	}
 	return 1;
 }
@@ -1881,11 +1882,20 @@ static void free_kmem_cache_nodes(struct kmem_cache *s)
 
 static int init_kmem_cache_nodes(struct kmem_cache *s, gfp_t gfpflags)
 {
-	init_kmem_cache_node(&s->local_node);
+	init_kmem_cache_node(&s->local_node, s);
 	return 1;
 }
 #endif
 
+static void set_min_partial(struct kmem_cache *s, unsigned long min)
+{
+	if (min < MIN_PARTIAL)
+		min = MIN_PARTIAL;
+	else if (min > MAX_PARTIAL)
+		min = MAX_PARTIAL;
+	s->min_partial = min;
+}
+
 /*
  * calculate_sizes() determines the order and the distribution of data within
  * a slab object.
@@ -2035,6 +2045,11 @@ static int kmem_cache_open(struct kmem_cache *s, gfp_t gfpflags,
 	if (!calculate_sizes(s))
 		goto error;
 
+	/*
+	 * The larger the object size is, the more pages we want on the partial
+	 * list to avoid pounding the page allocator excessively.
+	 */
+	set_min_partial(s, ilog2(s->size));
 	s->refcount = 1;
 #ifdef CONFIG_NUMA
 	s->remote_node_defrag_ratio = 100;
@@ -3312,6 +3327,25 @@ static ssize_t order_show(struct kmem_cache *s, char *buf)
 }
 SLAB_ATTR_RO(order);
 
+static ssize_t min_partial_show(struct kmem_cache *s, char *buf)
+{
+	return sprintf(buf, "%lu\n", s->min_partial);
+}
+
+static ssize_t min_partial_store(struct kmem_cache *s, const char *buf,
+				 size_t length)
+{
+	unsigned long min;
+
+	min = simple_strtoul(buf, NULL, 10);
+	if (min < 1)
+		return -EINVAL;
+
+	set_min_partial(s, min);
+	return length;
+}
+SLAB_ATTR(min_partial);
+
 static ssize_t ctor_show(struct kmem_cache *s, char *buf)
 {
 	if (s->ctor) {
@@ -3549,6 +3584,7 @@ static struct attribute * slab_attrs[] = {
 	&object_size_attr.attr,
 	&objs_per_slab_attr.attr,
 	&order_attr.attr,
+	&min_partial_attr.attr,
 	&objects_attr.attr,
 	&slabs_attr.attr,
 	&partial_attr.attr,
-- 
1.6.5.GIT
